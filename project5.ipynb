{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('un-general-debates.csv')\n",
    "\n",
    "# Drop NA values in the dataset\n",
    "data = data.dropna()\n",
    "\n",
    "# Get all the unique years\n",
    "dates = data['year'].unique()\n",
    "# print(dates.size)\n",
    "# print(dates.min(), dates.max())\n",
    "\n",
    "# Get all the unique countries\n",
    "countries = data['country'].unique()\n",
    "# print(countries)\n",
    "\n",
    "# How many times did each country speak at the UN?\n",
    "country_counts = data['country'].value_counts()\n",
    "print(country_counts)\n",
    "\n",
    "# Graph a histogram of the number of times each country spoke at the UN\n",
    "country_counts.hist(bins=len(country_counts))\n",
    "plt.xlabel('Number of Speeches')\n",
    "plt.ylabel('Number of Countries')\n",
    "plt.title('Number of Speeches by Country')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montenegro spoke the least in the UN. That makes sense, as they only joined in 2006.\n",
    "\n",
    "We only have data from 46 years (1970-2015). Most countries have spoken 45 or 46 times, which indicates that most countries in the UN have been in it since 1970 and were present for all general debates since then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the average word count of speeches by each country?\n",
    "data['wordCount'] = data['text'].str.split().str.len()\n",
    "average_word_count = data.groupby('country')['wordCount'].mean().round(0)\n",
    "\n",
    "# What is the average word count of speeches across all countries by year?\n",
    "average_word_count_all = data.groupby('year')['wordCount'].mean().round(0)\n",
    "print(f\"Average word count is: {average_word_count_all.mean()}\")\n",
    "\n",
    "# Print the average word count of speeches by each country\n",
    "# print(average_word_count)\n",
    "\n",
    "# Graph a histogram of the average word count of speeches by each country\n",
    "average_word_count.hist()\n",
    "plt.xlabel('Average Word Count')\n",
    "plt.ylabel('Number of Countries')\n",
    "plt.title('Average Word Count of Speeches by Country')\n",
    "plt.show()\n",
    "\n",
    "# Graph the average word count of speeches across all countries by year\n",
    "average_word_count_all.plot()\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Word Count')\n",
    "plt.title('Average Word Count of Speeches by Year')\n",
    "plt.show()\n",
    "\n",
    "# Graph the average word count of speeches by the US by year\n",
    "us_data = data[data['country'] == 'USA']\n",
    "us_data.groupby('year')['wordCount'].mean().plot()\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Word Count')\n",
    "plt.title('Average Word Count of Speeches by USA')\n",
    "plt.show()\n",
    "\n",
    "# Print the average word count of speeches by the United States\n",
    "us_word_count = data[data['country'] == 'USA']['wordCount']\n",
    "print(f\"US average word count: {average_word_count['USA']}\")\n",
    "\n",
    "# Save the bottom 5 countries with the lowest average word count\n",
    "bottom_5_countries = average_word_count.nsmallest(5)\n",
    "print(\"Bottom 5 countries with the lowest average word count:\")\n",
    "print(bottom_5_countries)\n",
    "\n",
    "# Save the bottom 1 country with the lowest average word count\n",
    "bottom_1_country = average_word_count.nsmallest(1)\n",
    "\n",
    "# Save the top 5 countries with the highest average word count\n",
    "top_5_countries = average_word_count.nlargest(5)\n",
    "print(\"Top 5 countries with the highest average word count:\")\n",
    "print(top_5_countries)\n",
    "\n",
    "# Save the top 1 country with the highest average word count\n",
    "top_1_country = average_word_count.nlargest(1)\n",
    "\n",
    "# Graph the average word count of bottom_1_country by year\n",
    "bottom_1_country_data = data[data['country'] == bottom_1_country.index[0]]\n",
    "bottom_1_country_data.groupby('year')['wordCount'].mean().plot()\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Word Count')\n",
    "plt.title(f'Average Word Count of Speeches by {bottom_1_country.index[0]}')\n",
    "plt.show()\n",
    "\n",
    "# Graph the average word count of top_1_country by year\n",
    "top_1_country_data = data[data['country'] == top_1_country.index[0]]\n",
    "top_1_country_data.groupby('year')['wordCount'].mean().plot()\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Word Count')\n",
    "plt.title(f'Average Word Count of Speeches by {top_1_country.index[0]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most countries hover around 3000 words per speech. The UN asks people to apply a voluntary 15-minute time limit, which is around 3000 words, so that makes sense.\n",
    "\n",
    "What happened in 1986 to tank Russia's word count so much?\n",
    "\n",
    "They got a new ambassador to the UN!\n",
    "\n",
    "Yakov Malik\n",
    "(1868-1976)\n",
    "\n",
    "Oleg Troyanovsky (1976-1986)\n",
    "\n",
    "Yuri Dubinin\n",
    "(1986-1986)\n",
    "\n",
    "Alexander Belonogov\n",
    "(1986-1990)\n",
    "\n",
    "OHOHOH, but also, there was a change in the USSR leadership to Mikhail Gorbachev, who was the leader from 1985-1991\n",
    "\n",
    "What happened in the US between 1995-2005?\n",
    "\n",
    "Not sure, lots of ambassadors. BUT, there was a change in ambassadors right around when the US started talking a lot again:\n",
    "\n",
    "Zalmay Khalilzad\n",
    "(2007-2009)\n",
    "\n",
    "Susan Rice\n",
    "(2009-2013)\n",
    "\n",
    "But the president was addressing the UN for the USA in 2009-2010 (and longer?), so it woudn't be because of Susan Rice.\n",
    "\n",
    "It was a presidency change, from Bush to Obama (Obama's first speech in the UN was in 2009). It seems that Obama talks more than Bush.\n",
    "\n",
    "<!-- Is the average word count even necessary?? Do countries speak more than once each year anyway? -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all of Russia's wordCounts for each year\n",
    "russia_data = data[data['country'] == 'RUS']\n",
    "\n",
    "# Order the russia_data by the year\n",
    "russia_data = russia_data.sort_values(by='year')\n",
    "# print(russia_data[['year', 'wordCount']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('un-general-debates.csv')\n",
    "\n",
    "# Word frequency\n",
    "vectorizer = CountVectorizer(stop_words='english')  # remove stop words -> cleaner frequency counts\n",
    "X = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "word_freq = np.asarray(X.sum(axis=0)).flatten()\n",
    "word_freq_df = pd.DataFrame({'word': vectorizer.get_feature_names_out(), 'frequency': word_freq})\n",
    "word_freq_df_sorted = word_freq_df.sort_values(by='frequency', ascending=False)\n",
    "\n",
    "# display the top 5 most common words\n",
    "top_5_words = word_freq_df_sorted.head(5)\n",
    "print(\"\\nTop 5 most common words and their frequencies:\")\n",
    "print(top_5_words)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data['country'], test_size=0.2, random_state=42)\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'\\nModel Performance:')\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data['text'])\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, data['country'], test_size=0.2, random_state=42)\n",
    "\n",
    "nb_classifier_tfidf = MultinomialNB()\n",
    "nb_classifier_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = nb_classifier_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "precision_tfidf = precision_score(y_test, y_pred_tfidf, average='weighted')\n",
    "recall_tfidf = recall_score(y_test, y_pred_tfidf, average='weighted')\n",
    "f1_tfidf = f1_score(y_test, y_pred_tfidf, average='weighted')\n",
    "\n",
    "print(f'TF-IDF Accuracy: {accuracy_tfidf:.2f}')\n",
    "print(f'TF-IDF Precision: {precision_tfidf:.2f}')\n",
    "print(f'TF-IDF Recall: {recall_tfidf:.2f}')\n",
    "print(f'TF-IDF F1-Score: {f1_tfidf:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# average word length\n",
    "def avg_word_length(text):\n",
    "    words = text.split()\n",
    "    return np.mean([len(word) for word in words]) if len(words) > 0 else 0\n",
    "\n",
    "data['avg_word_length'] = data['text'].apply(avg_word_length)\n",
    "\n",
    "X_word_length = data['avg_word_length'].values.reshape(-1, 1)\n",
    "\n",
    "X_train_word_length, X_test_word_length, y_train, y_test = train_test_split(X_word_length, data['country'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_classifier_word_length = GaussianNB()\n",
    "nb_classifier_word_length.fit(X_train_word_length, y_train)\n",
    "y_pred_word_length = nb_classifier_word_length.predict(X_test_word_length)\n",
    "\n",
    "accuracy_word_length = accuracy_score(y_test, y_pred_word_length)\n",
    "precision_word_length = precision_score(y_test, y_pred_word_length, average='weighted')\n",
    "recall_word_length = recall_score(y_test, y_pred_word_length, average='weighted')\n",
    "f1_word_length = f1_score(y_test, y_pred_word_length, average='weighted')\n",
    "\n",
    "print(f'Word Length Accuracy: {accuracy_word_length:.2f}')\n",
    "print(f'Word Length Precision: {precision_word_length:.2f}')\n",
    "print(f'Word Length Recall: {recall_word_length:.2f}')\n",
    "print(f'Word Length F1-Score: {f1_word_length:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lexical diversity\n",
    "def lexical_diversity(text):\n",
    "    words = text.split()\n",
    "    return len(set(words)) / len(words) if len(words) > 0 else 0\n",
    "\n",
    "data['lexical_diversity'] = data['text'].apply(lexical_diversity)\n",
    "\n",
    "X_lexical_diversity = data['lexical_diversity'].values.reshape(-1, 1)\n",
    "\n",
    "X_train_lexical_diversity, X_test_lexical_diversity, y_train, y_test = train_test_split(X_lexical_diversity, data['country'], test_size=0.2, random_state=42)\n",
    "\n",
    "#Gaussian Naive Bayes\n",
    "nb_classifier_lexical_diversity = GaussianNB()\n",
    "nb_classifier_lexical_diversity.fit(X_train_lexical_diversity, y_train)\n",
    "y_pred_lexical_diversity = nb_classifier_lexical_diversity.predict(X_test_lexical_diversity)\n",
    "\n",
    "accuracy_lexical_diversity = accuracy_score(y_test, y_pred_lexical_diversity)\n",
    "precision_lexical_diversity = precision_score(y_test, y_pred_lexical_diversity, average='weighted')\n",
    "recall_lexical_diversity = recall_score(y_test, y_pred_lexical_diversity, average='weighted')\n",
    "f1_lexical_diversity = f1_score(y_test, y_pred_lexical_diversity, average='weighted')\n",
    "\n",
    "print(f'Lexical Diversity Accuracy: {accuracy_lexical_diversity:.2f}')\n",
    "print(f'Lexical Diversity Precision: {precision_lexical_diversity:.2f}')\n",
    "print(f'Lexical Diversity Recall: {recall_lexical_diversity:.2f}')\n",
    "print(f'Lexical Diversity F1-Score: {f1_lexical_diversity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "methods = ['Word Frequency', 'TF-IDF', 'Average Word Length', 'Lexical Diversity']\n",
    "\n",
    "accuracies = [accuracy, accuracy_tfidf, accuracy_word_length, accuracy_lexical_diversity]\n",
    "precisions = [precision, precision_tfidf, precision_word_length, precision_lexical_diversity]\n",
    "recalls = [recall, recall_tfidf, recall_word_length, recall_lexical_diversity]\n",
    "f1_scores = [f1, f1_tfidf, f1_word_length, f1_lexical_diversity]\n",
    "\n",
    "bar_width = 0.2\n",
    "r1 = np.arange(len(methods))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "r3 = [x + bar_width for x in r2]\n",
    "r4 = [x + bar_width for x in r3]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(r1, accuracies, color='blue', width=bar_width, edgecolor='grey', label='Accuracy')\n",
    "plt.bar(r2, precisions, color='green', width=bar_width, edgecolor='grey', label='Precision')\n",
    "plt.bar(r3, recalls, color='red', width=bar_width, edgecolor='grey', label='Recall')\n",
    "plt.bar(r4, f1_scores, color='purple', width=bar_width, edgecolor='grey', label='F1-Score')\n",
    "\n",
    "plt.xlabel('Lexical Analysis Methods', fontweight='bold')\n",
    "plt.xticks([r + bar_width for r in range(len(methods))], methods)\n",
    "plt.ylabel('Scores', fontweight='bold')\n",
    "plt.title('Performance Comparison of Lexical Analysis Methods')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['country'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def map_country_to_region(country_code):\n",
    "    region_map = {\n",
    "        'Africa': ['NER', 'ZWE', 'SDN', 'MAR', 'EGY', 'RWA', 'MOZ', 'GAB', 'TCD', 'SYR', 'CMR', 'AGO', 'CIV', 'TGO',\n",
    "                   'LBR', 'LCA', 'MLI', 'GHA', 'GIN', 'GNB', 'BFA', 'SEN', 'STP', 'MRT', 'MLT', 'SWZ', 'ETH', 'MDG',\n",
    "                   'TUN', 'COD', 'GNQ', 'DZA', 'LSO', 'GMB', 'NGA', 'ZAF', 'MWI', 'DJI', 'BEN', 'SOM', 'ZMB', 'CPV',\n",
    "                   'COM', 'UGA', 'SLE', 'ERI', 'CAF', 'SSD'],\n",
    "        'Asia': ['MDV', 'PHL', 'RUS', 'CHN', 'MYS', 'NPL', 'BLR', 'BGD', 'JPN', 'KHM', 'IND', 'IDN', 'IRQ', 'IRN',\n",
    "                 'PAK', 'ISR', 'TUR', 'AFG', 'LKA', 'SGP', 'LAO', 'MMR', 'THA', 'JOR', 'SAU', 'QAT', 'KWT', 'TJK',\n",
    "                 'UZB', 'AZE', 'KAZ', 'TKM', 'KGZ', 'ARM', 'PSE', 'PRK', 'KOR', 'YEM', 'LBN', 'BTN', 'VNM'],\n",
    "        'Europe': ['FIN', 'ESP', 'PRT', 'BEL', 'ALB', 'GRC', 'LUX', 'ITA', 'BHR', 'CYP', 'NOR', 'ISL', 'UKR', 'FRA',\n",
    "                   'GBR', 'HUN', 'AUT', 'POL', 'BGR', 'ROU', 'NLD', 'DEU', 'DNK', 'YUG', 'CSK', 'BIH', 'HRV', 'SVK',\n",
    "                   'LTU', 'AND', 'TUR', 'LVA', 'CZE', 'SVN', 'CHE', 'VAT', 'MCO', 'MKD', 'LIE', 'GEO', 'EST', 'SMR', 'EU'],\n",
    "        'Americas': ['URY', 'ARG', 'SLV', 'COL', 'CAN', 'USA', 'MEX', 'BRA', 'PER', 'ECU', 'PAN', 'CUB', 'VEN', 'BOL',\n",
    "                     'HND', 'CRI', 'DOM', 'TTO', 'GTM', 'BLZ', 'GRD', 'JAM', 'HTI', 'BRB', 'BRN', 'PRY', 'BHS', 'SUR',\n",
    "                     'VCT', 'ATG', 'KNA'],\n",
    "        'Oceania': ['VUT', 'PNG', 'SLB', 'FJI', 'AUS', 'NZL', 'TON', 'NRU', 'TUV', 'FSM', 'KIR', 'WSM', 'MHL', 'PLW'],\n",
    "        'Middle East': ['SYR', 'ISR', 'IRN', 'IRQ', 'LBN', 'OMN', 'ARE', 'YEM', 'SAU', 'QAT', 'KWT', 'PSE', 'JOR'],\n",
    "    }\n",
    "    \n",
    "    for region, countries in region_map.items():\n",
    "        if country_code in countries:\n",
    "            return region\n",
    "    return 'Unknown'  # shouldn't happen I think\n",
    "\n",
    "data['region'] = data['country'].apply(map_country_to_region)\n",
    "\n",
    "X = data['text'] \n",
    "y_region = data['region']\n",
    "\n",
    "X_train_region, X_test_region, y_train_region, y_test_region = train_test_split(X, y_region, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text using TF-IDF\n",
    "tfidf_vectorizer_region = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf_region = tfidf_vectorizer_region.fit_transform(X_train_region)\n",
    "X_test_tfidf_region = tfidf_vectorizer_region.transform(X_test_region)\n",
    "\n",
    "nb_classifier_region = MultinomialNB()\n",
    "nb_classifier_region.fit(X_train_tfidf_region, y_train_region)\n",
    "y_pred_region = nb_classifier_region.predict(X_test_tfidf_region)\n",
    "\n",
    "accuracy_region = accuracy_score(y_test_region, y_pred_region)\n",
    "precision_region = precision_score(y_test_region, y_pred_region, average='weighted')\n",
    "recall_region = recall_score(y_test_region, y_pred_region, average='weighted')\n",
    "f1_region = f1_score(y_test_region, y_pred_region, average='weighted')\n",
    "\n",
    "print(f'Region Classification Accuracy: {accuracy_region:.2f}')\n",
    "print(f'Region Classification Precision: {precision_region:.2f}')\n",
    "print(f'Region Classification Recall: {recall_region:.2f}')\n",
    "print(f'Region Classification F1-Score: {f1_region:.2f}')\n",
    "\n",
    "# ------------------------- part 2 ----------------------------------------\n",
    "\n",
    "regions = data['region'].unique()\n",
    "results = []\n",
    "\n",
    "for region in regions:\n",
    "    region_data = data[data['region'] == region]\n",
    "    X_region = region_data['text']\n",
    "    y_country = region_data['country']\n",
    "    \n",
    "    X_train_country, X_test_country, y_train_country, y_test_country = train_test_split(X_region, y_country, test_size=0.2, random_state=42)\n",
    "    \n",
    "    tfidf_vectorizer_country = TfidfVectorizer(stop_words='english')\n",
    "    X_train_tfidf_country = tfidf_vectorizer_country.fit_transform(X_train_country)\n",
    "    X_test_tfidf_country = tfidf_vectorizer_country.transform(X_test_country)\n",
    "    \n",
    "    nb_classifier_country = MultinomialNB()\n",
    "    nb_classifier_country.fit(X_train_tfidf_country, y_train_country)\n",
    "    y_pred_country = nb_classifier_country.predict(X_test_tfidf_country)\n",
    "    \n",
    "    accuracy_country = accuracy_score(y_test_country, y_pred_country)\n",
    "    precision_country = precision_score(y_test_country, y_pred_country, average='weighted')\n",
    "    recall_country = recall_score(y_test_country, y_pred_country, average='weighted')\n",
    "    f1_country = f1_score(y_test_country, y_pred_country, average='weighted')\n",
    "    \n",
    "    results.append({\n",
    "        'region': region,\n",
    "        'accuracy': accuracy_country,\n",
    "        'precision': precision_country,\n",
    "        'recall': recall_country,\n",
    "        'f1': f1_country\n",
    "    })\n",
    "    \n",
    "    print(f'\\nResults for Region: {region}')\n",
    "    print(f'Country Prediction Accuracy: {accuracy_country:.2f}')\n",
    "    print(f'Country Prediction Precision: {precision_country:.2f}')\n",
    "    print(f'Country Prediction Recall: {recall_country:.2f}')\n",
    "    print(f'Country Prediction F1-Score: {f1_country:.2f}')\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nOverall Results by Region:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_results = results_df.groupby('region').mean().reset_index()\n",
    "region_results = region_results[region_results['region'] != 'Unknown']\n",
    "region_results.set_index('region', inplace=True)\n",
    "\n",
    "ax = region_results.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Overall Results by Region Using Regional Classification')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Metrics')\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
